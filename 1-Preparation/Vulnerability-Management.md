# Vulnerability Management

Vulnerability management is a continuous process of identifying, evaluating, treating, and reporting security vulnerabilities in systems and software. An effective vulnerability management program reduces organizational risk by proactively addressing weaknesses before they can be exploited.

<br>

## Tool Reference Table

| Tool | Type | Deployment | Pricing | Official Link |
|:-----|:-----|:-----------|:--------|:--------------|
| **Nessus** | Commercial Scanner | On-premise/Cloud | Paid (Free Essentials) | [tenable.com](https://www.tenable.com/products/nessus) |
| **OpenVAS** | Open Source Scanner | On-premise | Free | [greenbone.net](https://www.greenbone.net/en/) |
| **Qualys VMDR** | Cloud Platform | SaaS | Paid | [qualys.com](https://www.qualys.com/) |
| **Rapid7 Nexpose** | Commercial Scanner | On-premise/Cloud | Paid | [rapid7.com](https://www.rapid7.com/products/nexpose/) |
| **Nikto** | Web Scanner | On-premise | Free | [cirt.net/Nikto2](https://cirt.net/Nikto2) |

<br>
<br>

## Vulnerability Management Lifecycle

Understanding the vulnerability management lifecycle is essential for building an effective program:

### 1. **Asset Discovery & Inventory**
Identify and catalog all assets in your environment (servers, workstations, network devices, cloud resources).

### 2. **Vulnerability Assessment**
Scan assets for known vulnerabilities using automated tools and manual testing.

### 3. **Risk Prioritization**
Evaluate vulnerabilities based on severity, exploitability, business impact, and asset criticality.

### 4. **Remediation**
Apply patches, configuration changes, or compensating controls to address vulnerabilities.

### 5. **Verification**
Confirm vulnerabilities have been successfully remediated through rescanning.

### 6. **Reporting & Metrics**
Track progress, measure effectiveness, and report to stakeholders.

<br>

## Nessus
Nessus is the industry-leading vulnerability scanner developed by Tenable. It provides comprehensive vulnerability detection across on-premise, cloud, and containerized environments with extensive plugin coverage and accurate detection capabilities.

### Key Features

- **70,000+ Vulnerability Plugins** - Comprehensive coverage of CVEs
- **Compliance Auditing** - PCI-DSS, HIPAA, CIS benchmarks, DISA STIGs
- **Credentialed Scanning** - Deep system inspection with authentication
- **Web Application Scanning** - Basic web vulnerability detection
- **Configuration Auditing** - Identify misconfigurations and weak settings
- **Live Results** - Real-time vulnerability detection during scans
- **Advanced Reporting** - Customizable reports and executive dashboards

### Editions Comparison

| Feature | Nessus Essentials | Nessus Professional | Nessus Expert |
|:--------|:------------------|:--------------------|:--------------|
| **Assets** | Up to 16 | Unlimited | Unlimited |
| **Cost** | Free | ~$4,000/year | ~$6,000/year |
| **Use Case** | Home/Education | Enterprise networks | Modern attack surface |
| **Web App Scanning** | âœ“ | âœ“ | âœ“ |
| **Cloud Scanning** | âœ— | âœ“ | âœ“ |
| **Container Scanning** | âœ— | âœ— | âœ“ |
| **Advanced Support** | Community | Standard | Priority |

### Installation & Setup

**Linux Installation:**
```bash
# Download Nessus (replace with current version)
wget https://www.tenable.com/downloads/api/v1/public/pages/nessus/downloads/18493/download?i_agree_to_tenable_license_agreement=true -O Nessus-10.7.0-ubuntu1404_amd64.deb

# Install package
sudo dpkg -i Nessus-10.7.0-ubuntu1404_amd64.deb

# Start Nessus service
sudo systemctl start nessusd
sudo systemctl enable nessusd

# Access web interface
# URL: https://localhost:8834
# Initial setup requires activation code

# Check service status
sudo systemctl status nessusd
```

**Windows Installation:**
```powershell
# Download installer from Tenable website
# Run installer: Nessus-10.7.0-x64.msi

# Start Nessus service
Start-Service "Tenable Nessus"

# Access web interface
# URL: https://localhost:8834
```

**Docker Installation:**
```bash
# Pull Nessus Docker image
docker pull tenable/nessus:latest-ubuntu

# Run Nessus container
docker run -d \
  --name nessus \
  -p 8834:8834 \
  -v nessus-data:/opt/nessus/var/nessus \
  tenable/nessus:latest-ubuntu

# Access at https://localhost:8834
```

**Initial Configuration:**
```bash
# After accessing web interface:
# 1. Create admin account
# 2. Enter activation code (from Tenable)
# 3. Download plugins (may take 30-60 minutes)
# 4. Configure scan settings

# Plugin update schedule (via CLI)
sudo /opt/nessus/sbin/nessuscli update --all

# View plugin information
sudo /opt/nessus/sbin/nessuscli plugin list
```

### Practical Examples

**Create and configure a basic scan:**
```bash
# Using Nessus API (requires API keys from Settings > API Keys)

# Generate API keys via web UI first
# Then use curl or Python to interact

# Example: List all scans
curl -k -X GET \
  https://localhost:8834/scans \
  -H "X-ApiKeys: accessKey=YOUR_ACCESS_KEY; secretKey=YOUR_SECRET_KEY"
```

**Python API Example - Create and Launch Scan:**
```python
import requests
import json
import time

# Disable SSL warnings for self-signed cert
requests.packages.urllib3.disable_warnings()

class NessusAPI:
    def launch_scan(self, scan_title, ip_list, option_id=None):
        """Launch a vulnerability scan"""
        url = f'{self.base_url}/api/2.0/fo/scan/'
        
        params = {
            'action': 'launch',
            'scan_title': scan_title,
            'ip': ip_list,
            'option_id': option_id or 45  # Default: Initial Options
        }
        
        response = self.session.post(url, data=params)
        return response.text
    
    def get_scan_results(self, scan_ref):
        """Get scan results"""
        url = f'{self.base_url}/api/2.0/fo/scan/'
        
        params = {
            'action': 'fetch',
            'scan_ref': scan_ref,
            'mode': 'extended',
            'output_format': 'xml'
        }
        
        response = self.session.post(url, data=params)
        return response.text
    
    def get_host_detections(self, ip_address):
        """Get vulnerabilities for specific host"""
        url = f'{self.base_url}/api/2.0/fo/asset/host/vm/detection/'
        
        params = {
            'action': 'list',
            'ips': ip_address,
            'status': 'New,Active,Re-Opened',
            'truncation_limit': 1000
        }
        
        response = self.session.post(url, data=params)
        return response.text
    
    def get_vulnerability_details(self, qid):
        """Get details for specific vulnerability QID"""
        url = f'{self.base_url}/api/2.0/fo/knowledge_base/vuln/'
        
        params = {
            'action': 'list',
            'details': 'All',
            'ids': qid
        }
        
        response = self.session.post(url, data=params)
        return response.text

# Usage Example
qualys = QualysAPI(
    username='your_username',
    password='your_password',
    api_server='qualysapi.qualys.com'
)

# Launch scan
scan_response = qualys.launch_scan(
    scan_title='Weekly Production Scan',
    ip_list='192.168.1.0-192.168.1.255',
    option_id=45
)

# Parse response to get scan reference
root = ET.fromstring(scan_response)
scan_ref = root.find('.//REFERENCE').text
print(f"Scan launched: {scan_ref}")
```

**Automated Vulnerability Reporting:**
```python
import requests
from requests.auth import HTTPBasicAuth
import xml.etree.ElementTree as ET
import pandas as pd
from datetime import datetime

class QualysReporter:
    def __init__(self, username, password, api_server='qualysapi.qualys.com'):
        self.api = QualysAPI(username, password, api_server)
    
    def generate_executive_report(self, asset_group_id=None):
        """Generate executive summary of vulnerabilities"""
        url = f'{self.api.base_url}/api/2.0/fo/asset/host/vm/detection/'
        
        params = {
            'action': 'list',
            'status': 'New,Active,Re-Opened',
            'severities': '5,4,3,2,1',
            'truncation_limit': 10000
        }
        
        if asset_group_id:
            params['ag_ids'] = asset_group_id
        
        response = self.api.session.post(url, data=params)
        root = ET.fromstring(response.text)
        
        # Parse vulnerabilities
        vulnerabilities = []
        for detection in root.findall('.//DETECTION'):
            vuln = {
                'host': detection.find('.//IP').text if detection.find('.//IP') is not None else 'N/A',
                'qid': detection.find('.//QID').text,
                'severity': detection.find('.//SEVERITY').text,
                'title': detection.find('.//TITLE').text if detection.find('.//TITLE') is not None else '',
                'first_found': detection.find('.//FIRST_FOUND_DATETIME').text,
                'last_found': detection.find('.//LAST_FOUND_DATETIME').text
            }
            vulnerabilities.append(vuln)
        
        # Create DataFrame
        df = pd.DataFrame(vulnerabilities)
        
        # Generate statistics
        stats = {
            'total_vulnerabilities': len(df),
            'critical': len(df[df['severity'] == '5']),
            'high': len(df[df['severity'] == '4']),
            'medium': len(df[df['severity'] == '3']),
            'low': len(df[df['severity'] == '2']),
            'info': len(df[df['severity'] == '1']),
            'unique_hosts': df['host'].nunique(),
            'report_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        return stats, df
    
    def export_to_csv(self, dataframe, filename='qualys_vulnerabilities.csv'):
        """Export vulnerabilities to CSV"""
        dataframe.to_csv(filename, index=False)
        print(f"Report exported to {filename}")

# Usage
reporter = QualysReporter('username', 'password')
stats, vuln_df = reporter.generate_executive_report()

print("=== Vulnerability Summary ===")
print(f"Total Vulnerabilities: {stats['total_vulnerabilities']}")
print(f"Critical: {stats['critical']}")
print(f"High: {stats['high']}")
print(f"Medium: {stats['medium']}")
print(f"Low: {stats['low']}")
print(f"Affected Hosts: {stats['unique_hosts']}")

# Export to CSV
reporter.export_to_csv(vuln_df)
```

**Integration with ServiceNow for Ticket Creation:**
```python
import requests
from requests.auth import HTTPBasicAuth
import json

class QualysServiceNowIntegration:
    def __init__(self, qualys_user, qualys_pass, snow_instance, snow_user, snow_pass):
        self.qualys = QualysAPI(qualys_user, qualys_pass)
        self.snow_url = f'https://{snow_instance}.service-now.com/api/now/table/incident'
        self.snow_auth = HTTPBasicAuth(snow_user, snow_pass)
        self.snow_headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
    
    def create_tickets_for_critical_vulns(self):
        """Create ServiceNow tickets for critical vulnerabilities"""
        # Get critical and high vulnerabilities
        url = f'{self.qualys.base_url}/api/2.0/fo/asset/host/vm/detection/'
        params = {
            'action': 'list',
            'status': 'New,Active',
            'severities': '5,4',  # Critical and High only
            'truncation_limit': 1000
        }
        
        response = self.qualys.session.post(url, data=params)
        root = ET.fromstring(response.text)
        
        tickets_created = 0
        for detection in root.findall('.//DETECTION'):
            host = detection.find('.//IP').text
            qid = detection.find('.//QID').text
            severity = detection.find('.//SEVERITY').text
            title = detection.find('.//TITLE').text if detection.find('.//TITLE') is not None else 'Unknown'
            
            # Create ServiceNow ticket
            ticket_data = {
                'short_description': f'Qualys: {title} on {host}',
                'description': f'Vulnerability QID: {qid}\nSeverity: {severity}\nHost: {host}',
                'urgency': '1' if severity == '5' else '2',
                'impact': '1' if severity == '5' else '2',
                'category': 'Security',
                'subcategory': 'Vulnerability',
                'assignment_group': 'Security Operations'
            }
            
            response = requests.post(
                self.snow_url,
                auth=self.snow_auth,
                headers=self.snow_headers,
                json=ticket_data
            )
            
            if response.status_code == 201:
                tickets_created += 1
                ticket_number = response.json()['result']['number']
                print(f"Created ticket {ticket_number} for QID {qid} on {host}")
        
        return tickets_created

# Usage
integration = QualysServiceNowIntegration(
    qualys_user='qualys_user',
    qualys_pass='qualys_pass',
    snow_instance='yourcompany',
    snow_user='snow_user',
    snow_pass='snow_pass'
)

tickets = integration.create_tickets_for_critical_vulns()
print(f"Total tickets created: {tickets}")
```

**Compliance Dashboard:**
```python
import requests
from requests.auth import HTTPBasicAuth
import xml.etree.ElementTree as ET
import matplotlib.pyplot as plt
import seaborn as sns

class QualysCompliance:
    def __init__(self, username, password):
        self.api = QualysAPI(username, password)
    
    def get_pci_compliance_status(self):
        """Get PCI-DSS compliance posture"""
        url = f'{self.api.base_url}/api/2.0/fo/compliance/posture/'
        
        params = {
            'action': 'list',
            'compliance_type': 'pci'
        }
        
        response = self.api.session.post(url, data=params)
        return response.text
    
    def visualize_compliance_trends(self, data):
        """Create compliance trend visualization"""
        # Parse compliance data
        root = ET.fromstring(data)
        
        compliance_scores = []
        dates = []
        
        for posture in root.findall('.//POSTURE'):
            date = posture.find('.//DATE').text
            score = float(posture.find('.//SCORE').text)
            compliance_scores.append(score)
            dates.append(date)
        
        # Create plot
        plt.figure(figsize=(12, 6))
        plt.plot(dates, compliance_scores, marker='o', linewidth=2)
        plt.axhline(y=100, color='g', linestyle='--', label='Full Compliance')
        plt.title('PCI-DSS Compliance Trend', fontsize=16)
        plt.xlabel('Date')
        plt.ylabel('Compliance Score (%)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig('compliance_trend.png')
        print("Compliance trend chart saved as compliance_trend.png")

# Usage
compliance = QualysCompliance('username', 'password')
pci_data = compliance.get_pci_compliance_status()
compliance.visualize_compliance_trends(pci_data)
```

### Best Practices

- **Asset Tagging** - Use tags to organize assets by criticality, location, or business unit
- **Scan Optimization** - Use Cloud Agents for endpoints, scanners for servers and network devices
- **Progressive Scanning** - Start with discovery scans, then move to authenticated scans
- **Remediation Workflow** - Integrate with patch management and ticketing systems
- **TruRisk Scoring** - Prioritize based on contextual risk, not just CVSS
- **Compliance Mapping** - Use Policy Compliance module for regulatory requirements
- **API Automation** - Leverage APIs for custom integrations and reporting
- **Multi-Cloud Strategy** - Deploy connectors for all cloud providers in use

<br>

## Vulnerability Management Program

### Building a Mature Program

**Program Components:**

1. **Asset Management**
   - Maintain accurate inventory
   - Classify assets by criticality
   - Track asset owners

2. **Scanning Strategy**
   - Authenticated vs. unauthenticated scans
   - Scan frequency based on asset criticality
   - Network segmentation considerations

3. **Risk Assessment**
   - CVSS base scoring
   - Threat intelligence integration
   - Business context (asset criticality, exposure)
   - Exploit availability

4. **Remediation Workflow**
   - SLA-based prioritization
   - Ticket assignment and tracking
   - Patch management integration
   - Compensating controls documentation

5. **Metrics & KPIs**
   - Mean Time To Detect (MTTD)
   - Mean Time To Remediate (MTTR)
   - Vulnerability density (vulns per asset)
   - Remediation rate by severity
   - Scan coverage percentage

### Risk-Based Prioritization Framework

```
Priority = (Severity Ã— Exploitability Ã— Asset Value) / Mitigating Controls

Severity Levels:
- Critical (10): Remote code execution, privilege escalation
- High (7-9): Information disclosure, DoS
- Medium (4-6): Configuration issues, weak encryption
- Low (1-3): Informational findings

Exploitability:
- Active exploits in the wild: 3x
- Public exploit code available: 2x
- Proof of concept exists: 1.5x
- No known exploits: 1x

Asset Value:
- Tier 1 (Critical): Customer data, financial systems (3x)
- Tier 2 (High): Internal apps, workstations (2x)
- Tier 3 (Medium): Development, test systems (1x)

Mitigating Controls:
- Network segmentation: 0.5x reduction
- WAF/IPS in place: 0.7x reduction
- MFA enabled: 0.8x reduction
```

### Remediation SLAs

| Priority | Target Remediation Time | Example Vulnerabilities |
|:---------|:------------------------|:------------------------|
| **P0 - Emergency** | 24 hours | Actively exploited zero-days |
| **P1 - Critical** | 7 days | RCE on internet-facing systems |
| **P2 - High** | 30 days | Privilege escalation on internal systems |
| **P3 - Medium** | 90 days | Information disclosure, configuration issues |
| **P4 - Low** | 180 days | Informational findings, hardening recommendations |

### Vulnerability Lifecycle Management

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Discovery   â”‚ â† Automated scanning, threat intel, bug bounty
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Validation  â”‚ â† False positive check, manual verification
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prioritizationâ”‚ â† Risk scoring, business impact assessment
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Assignment  â”‚ â† Ticket creation, owner notification
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Remediation  â”‚ â† Patching, configuration change, mitigation
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Verification â”‚ â† Rescan, testing, sign-off
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Closure    â”‚ â† Documentation, lessons learned
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Advanced Scanning Techniques

### Network Segmentation Testing

**Verify Firewall Rules:**
```bash
# Scan from external network
nmap -sS -p 1-65535 --open external-server.com

# Scan from internal network
nmap -sS -p 1-65535 --open 192.168.1.100

# Compare results to validate segmentation
```

**Zone Testing Strategy:**
```
1. Internet â†’ DMZ (should be limited)
2. DMZ â†’ Internal (should be blocked)
3. Internal â†’ DMZ (should be controlled)
4. Internal â†’ Internal (depends on policy)
5. Guest â†’ Internal (should be blocked)
```

### Web Application Scanning

**Comprehensive Web App Testing:**
```bash
# Using Nikto for web server vulnerabilities
nikto -h https://webapp.company.com -output nikto_report.html -Format html

# OWASP ZAP automated scan
zap-cli quick-scan https://webapp.company.com

# Custom SSL/TLS testing
sslscan --show-certificate webapp.company.com:443
testssl.sh --full https://webapp.company.com
```

### Container and Cloud Scanning

**Docker Container Scanning:**
```bash
# Using Trivy
trivy image myapp:latest

# Using Clair
docker run -d --name clair-db postgres
docker run -d --name clair --link clair-db:postgres clair

# Using Anchore
anchore-cli image add myapp:latest
anchore-cli image vuln myapp:latest all
```

**Kubernetes Security Scanning:**
```bash
# Using kube-bench (CIS benchmarks)
kube-bench run --targets master,node

# Using kubesec
kubesec scan deployment.yaml

# Using Falco for runtime security
falco -c /etc/falco/falco.yaml
```

<br>

## Reporting & Metrics

### Key Performance Indicators

**Operational Metrics:**
```python
# Calculate key vulnerability metrics
def calculate_vuln_metrics(vulnerabilities):
    """Calculate standard vulnerability metrics"""
    
    from datetime import datetime, timedelta
    
    metrics = {
        'total_vulnerabilities': len(vulnerabilities),
        'by_severity': {
            'critical': 0,
            'high': 0,
            'medium': 0,
            'low': 0
        },
        'mean_time_to_detect': 0,
        'mean_time_to_remediate': 0,
        'overdue_vulns': 0,
        'vulnerability_density': 0
    }
    
    total_detection_time = timedelta()
    total_remediation_time = timedelta()
    remediated_count = 0
    
    for vuln in vulnerabilities:
        # Count by severity
        severity = vuln['severity'].lower()
        if severity in metrics['by_severity']:
            metrics['by_severity'][severity] += 1
        
        # Calculate MTTD
        if vuln.get('discovered_date') and vuln.get('asset_scan_date'):
            detection_time = vuln['discovered_date'] - vuln['asset_scan_date']
            total_detection_time += detection_time
        
        # Calculate MTTR
        if vuln.get('remediated_date') and vuln.get('discovered_date'):
            remediation_time = vuln['remediated_date'] - vuln['discovered_date']
            total_remediation_time += remediation_time
            remediated_count += 1
        
        # Check SLA compliance
        if vuln.get('due_date') and not vuln.get('remediated_date'):
            if datetime.now() > vuln['due_date']:
                metrics['overdue_vulns'] += 1
    
    # Calculate averages
    if len(vulnerabilities) > 0:
        metrics['mean_time_to_detect'] = (total_detection_time / len(vulnerabilities)).days
    
    if remediated_count > 0:
        metrics['mean_time_to_remediate'] = (total_remediation_time / remediated_count).days
    
    return metrics

# Example usage
vuln_data = [
    {
        'severity': 'critical',
        'discovered_date': datetime(2024, 1, 1),
        'asset_scan_date': datetime(2023, 12, 31),
        'remediated_date': datetime(2024, 1, 5),
        'due_date': datetime(2024, 1, 7)
    },
    # ... more vulnerabilities
]

metrics = calculate_vuln_metrics(vuln_data)
print(f"MTTD: {metrics['mean_time_to_detect']} days")
print(f"MTTR: {metrics['mean_time_to_remediate']} days")
print(f"Overdue: {metrics['overdue_vulns']}")
```

### Executive Dashboard

**Visualization Example:**
```python
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

def create_executive_dashboard(metrics_data):
    """Create executive vulnerability dashboard"""
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Vulnerability Management Dashboard', fontsize=20, fontweight='bold')
    
    # 1. Vulnerability Trend Over Time
    ax1 = axes[0, 0]
    trend_df = pd.DataFrame(metrics_data['trends'])
    ax1.plot(trend_df['date'], trend_df['critical'], label='Critical', color='red', linewidth=2)
    ax1.plot(trend_df['date'], trend_df['high'], label='High', color='orange', linewidth=2)
    ax1.plot(trend_df['date'], trend_df['medium'], label='Medium', color='yellow', linewidth=2)
    ax1.set_title('Vulnerability Trends', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Current Distribution by Severity
    ax2 = axes[0, 1]
    severity_data = metrics_data['current_severity']
    colors = ['#d32f2f', '#f57c00', '#fbc02d', '#388e3c']
    ax2.pie(severity_data.values(), labels=severity_data.keys(), autopct='%1.1f%%', 
            colors=colors, startangle=90)
    ax2.set_title('Current Vulnerability Distribution', fontsize=14)
    
    # 3. MTTR by Severity
    ax3 = axes[1, 0]
    mttr_data = metrics_data['mttr_by_severity']
    bars = ax3.bar(mttr_data.keys(), mttr_data.values(), color=colors)
    ax3.set_title('Mean Time To Remediate (Days)', fontsize=14)
    ax3.set_ylabel('Days')
    ax3.axhline(y=30, color='green', linestyle='--', label='Target')
    ax3.legend()
    
    # 4. Top 10 Vulnerable Assets
    ax4 = axes[1, 1]
    top_assets = metrics_data['top_vulnerable_assets']
    ax4.barh(list(top_assets.keys()), list(top_assets.values()), color='steelblue')
    ax4.set_title('Top 10 Vulnerable Assets', fontsize=14)
    ax4.set_xlabel('Number of Vulnerabilities')
    
    plt.tight_layout()
    plt.savefig('vuln_dashboard.png', dpi=300, bbox_inches='tight')
    print("Dashboard saved as vuln_dashboard.png")

# Example usage with sample data
dashboard_data = {
    'trends': {
        'date': pd.date_range('2024-01-01', periods=12, freq='M'),
        'critical': [45, 42, 38, 35, 32, 28, 25, 22, 20, 18, 15, 12],
        'high': [120, 115, 110, 105, 100, 95, 90, 85, 80, 75, 70, 65],
        'medium': [250, 245, 240, 235, 230, 225, 220, 215, 210, 205, 200, 195]
    },
    'current_severity': {
        'Critical': 12,
        'High': 65,
        'Medium': 195,
        'Low': 320
    },
    'mttr_by_severity': {
        'Critical': 5,
        'High': 18,
        'Medium': 45,
        'Low': 90
    },
    'top_vulnerable_assets': {
        'web-server-01': 45,
        'db-server-03': 38,
        'app-server-02': 35,
        'web-server-04': 32,
        'api-gateway-01': 28,
        'mail-server-01': 25,
        'file-server-02': 22,
        'web-server-05': 20,
        'db-server-01': 18,
        'app-server-05': 15
    }
}

create_executive_dashboard(dashboard_data)
```

---

## ğŸ› ï¸ Additional Tools & Resources

### Open Source Scanners
- **Nuclei** - Fast vulnerability scanner with template-based detection
- **Nmap** - Network discovery and security auditing
- **Lynis** - Security auditing tool for Unix/Linux
- **OWASP ZAP** - Web application security scanner
- **Trivy** - Container vulnerability scanner

### Commercial Platforms
- **Rapid7 InsightVM** - Unified vulnerability management
- **Tenable.io** - Cloud-based vulnerability management
- **CrowdStrike Spotlight** - Continuous vulnerability assessment
- **Microsoft Defender for Endpoint** - Integrated threat and vulnerability management

### Compliance & Frameworks
- **NIST SP 800-40** - Guide to Enterprise Patch Management
- **PCI-DSS Requirement 11** - Vulnerability scanning requirements
- **ISO 27001 A.12.6** - Technical vulnerability management
- **CIS Controls 7** - Continuous Vulnerability Management

### Useful Resources
- [FIRST CVSS Calculator](https://www.first.org/cvss/calculator/3.1)
- [NVD - National Vulnerability Database](https://nvd.nist.gov/)
- [CISA Known Exploited Vulnerabilities](https://www.cisa.gov/known-exploited-vulnerabilities-catalog)
- [Exploit Database](https://www.exploit-db.com/)
- [MITRE CVE List](https://cve.mitre.org/)

<br>

## Common Pitfalls & Solutions

### Pitfall 1: Scan Overload
**Problem:** Too many scans causing network performance issues
**Solution:**
- Schedule scans during maintenance windows
- Use incremental scanning instead of full scans
- Implement scan throttling
- Deploy distributed scanners

### Pitfall 2: False Positive Fatigue
**Problem:** High volume of false positives reducing trust in results
**Solution:**
- Use credentialed scanning for accuracy
- Implement a validation process
- Tune scanner configurations
- Maintain a false positive database

### Pitfall 3: Lack of Prioritization
**Problem:** Treating all vulnerabilities equally leads to ineffective remediation
**Solution:**
- Implement risk-based prioritization
- Consider threat intelligence
- Factor in asset criticality
- Use EPSS (Exploit Prediction Scoring System)

### Pitfall 4: Remediation Bottlenecks
**Problem:** Vulnerabilities pile up faster than they can be fixed
**Solution:**
- Automate patch deployment where possible
- Implement virtual patching with WAF/IPS
- Use compensating controls
- Negotiate realistic SLAs

### Pitfall 5: Poor Asset Inventory
**Problem:** Scanning incomplete or inaccurate asset inventory
**Solution:**
- Implement continuous asset discovery
- Use network access control (NAC)
- Deploy agents on endpoints
- Integrate with CMDB

<br>

## Training & Certifications

### Relevant Certifications
- **GIAC Vulnerability Assessment (GVAC)** - Vulnerability assessment methodology
- **Certified Vulnerability Assessor (CVA)** - Mile2 certification
- **CSSLP** - Secure Software Lifecycle Professional
- **CEH** - Certified Ethical Hacker (includes vulnerability assessment)

### Training Resources
- **SANS SEC460** - Enterprise Vulnerability Assessment
- **Offensive Security AWE** - Advanced Web Exploitation
- **Tenable University** - Nessus and Tenable.io training
- **Qualys Training** - Platform-specific courses

<br>

## Maturity Model

### Level 1 - Initial
- Ad-hoc scanning
- No formal process
- Manual reporting
- Reactive remediation

### Level 2 - Developing
- Regular scan schedules
- Basic prioritization
- Defined SLAs
- Ticket-based tracking

### Level 3 - Defined
- Risk-based prioritization
- Automated reporting
- Integrated workflows
- Metrics tracking

### Level 4 - Managed
- Continuous assessment
- Threat intelligence integration
- Automated remediation
- Comprehensive metrics

### Level 5 - Optimized
- Predictive analytics
- Full automation
- Proactive risk management
- Industry leadership

<br>

</div> __init__(self, url, access_key, secret_key):
        self.url = url
        self.headers = {
            'X-ApiKeys': f'accessKey={access_key}; secretKey={secret_key}',
            'Content-Type': 'application/json'
        }
    
    def create_scan(self, name, description, targets, template_uuid):
        """Create a new scan"""
        payload = {
            'uuid': template_uuid,
            'settings': {
                'name': name,
                'description': description,
                'text_targets': targets,
                'enabled': True
            }
        }
        
        response = requests.post(
            f'{self.url}/scans',
            headers=self.headers,
            json=payload,
            verify=False
        )
        return response.json()
    
    def launch_scan(self, scan_id):
        """Launch a scan"""
        response = requests.post(
            f'{self.url}/scans/{scan_id}/launch',
            headers=self.headers,
            verify=False
        )
        return response.json()
    
    def get_scan_status(self, scan_id):
        """Check scan status"""
        response = requests.get(
            f'{self.url}/scans/{scan_id}',
            headers=self.headers,
            verify=False
        )
        return response.json()
    
    def export_scan(self, scan_id, format='pdf'):
        """Export scan results"""
        # Request export
        payload = {'format': format}
        response = requests.post(
            f'{self.url}/scans/{scan_id}/export',
            headers=self.headers,
            json=payload,
            verify=False
        )
        file_id = response.json()['file']
        
        # Wait for export to complete
        while True:
            status = requests.get(
                f'{self.url}/scans/{scan_id}/export/{file_id}/status',
                headers=self.headers,
                verify=False
            ).json()
            
            if status['status'] == 'ready':
                break
            time.sleep(5)
        
        # Download export
        download = requests.get(
            f'{self.url}/scans/{scan_id}/export/{file_id}/download',
            headers=self.headers,
            verify=False
        )
        return download.content

<br>

# Usage example
nessus = NessusAPI(
    url='https://localhost:8834',
    access_key='YOUR_ACCESS_KEY',
    secret_key='YOUR_SECRET_KEY'
)

> Get template UUID (use 'Basic Network Scan' template)
> Template UUID can be found via: GET /editor/scan/templates

<br>

# Create scan
scan = nessus.create_scan(
    name='Production Network Scan',
    description='Weekly vulnerability scan of production servers',
    targets='192.168.1.0/24, 10.0.1.10-20',
    template_uuid='731a8e52-3ea6-a291-ec0a-d2ff0619c19d7bd788d6be818b65'
)

print(f"Created scan ID: {scan['scan']['id']}")

<br>

# Launch scan
nessus.launch_scan(scan['scan']['id'])
print("Scan launched")

<br>

# Monitor status
scan_id = scan['scan']['id']
while True:
    status = nessus.get_scan_status(scan_id)
    print(f"Status: {status['info']['status']}")
    
    if status['info']['status'] == 'completed':
        break
    
    time.sleep(30)

<br>

# Export results
pdf_report = nessus.export_scan(scan_id, format='pdf')
with open('scan_report.pdf', 'wb') as f:
    f.write(pdf_report)

print("Scan completed and exported")
```

**Credentialed Scanning Configuration:**
```python
# Add credentials for deeper scanning
credentials_payload = {
    'uuid': template_uuid,
    'settings': {
        'name': 'Credentialed Scan',
        'text_targets': '192.168.1.0/24',
    },
    'credentials': {
        'add': {
            'Host': {
                'Windows': [{
                    'domain': 'CORP',
                    'username': 'scanner_account',
                    'password': 'SecurePassword123!',
                    'auth_method': 'Password'
                }],
                'SSH': [{
                    'username': 'scanner',
                    'password': 'SecurePassword123!',
                    'elevate_privileges_with': 'sudo',
                    'escalation_password': 'SecurePassword123!'
                }]
            }
        }
    }
}

# Create credentialed scan
cred_scan = requests.post(
    'https://localhost:8834/scans',
    headers=headers,
    json=credentials_payload,
    verify=False
).json()
```

**Advanced Scan Configuration:**
```python
# Configure advanced scan options
advanced_scan = {
    'uuid': template_uuid,
    'settings': {
        'name': 'Advanced Vulnerability Scan',
        'description': 'Comprehensive scan with web app testing',
        'folder_id': 3,
        'scanner_id': 1,
        'policy_id': 4,
        'text_targets': 'webapp.company.com, 192.168.1.0/24',
        'file_targets': '',
        
        # Timing
        'scan_time_window': 'RRULE:FREQ=WEEKLY;BYDAY=SU',
        'starttime': '20240114T020000',
        
        # Performance
        'max_checks_per_host': 5,
        'max_hosts_per_scan': 100,
        'network_receive_timeout': 5,
        'network_connect_timeout': 5,
        
        # Discovery
        'ping_the_remote_host': 'yes',
        'tcp_ping': 'yes',
        'syn_scanner': 'yes',
        'port_range': '1-65535',
        
        # Assessment
        'enable_plugin_debugging': 'no',
        'safe_checks': 'yes',
        'stop_scan_on_disconnect': 'no',
        'slice_network_addresses': 'no',
        
        # Web Applications
        'enable_web_app_tests': 'yes',
        'web_app_test_mode': 'crawl_and_audit',
        
        # Reporting
        'log_live_results': 'yes',
    }
}
```

**Automated Scanning Schedule:**
```python
import schedule
import time

def run_weekly_scan():
    """Automated weekly scanning function"""
    nessus = NessusAPI(
        url='https://localhost:8834',
        access_key='YOUR_ACCESS_KEY',
        secret_key='YOUR_SECRET_KEY'
    )
    
    # Launch predefined scan
    scan_id = 123  # Your scan ID
    result = nessus.launch_scan(scan_id)
    print(f"Launched scan {scan_id} at {time.ctime()}")
    
    # Wait for completion
    while True:
        status = nessus.get_scan_status(scan_id)
        if status['info']['status'] == 'completed':
            # Export and email results
            pdf_report = nessus.export_scan(scan_id, format='pdf')
            # Send email with attachment (implement email logic)
            break
        time.sleep(300)  # Check every 5 minutes

# Schedule scan for Sunday at 2 AM
schedule.every().sunday.at("02:00").do(run_weekly_scan)

while True:
    schedule.run_pending()
    time.sleep(3600)  # Check every hour
```

<br>

### Best Practices

- **Credentialed Scans** - Always use credentials for accurate results and fewer false positives
- **Scan Scheduling** - Run scans during maintenance windows to avoid performance impact
- **Target Segmentation** - Scan in logical groups (production, development, DMZ)
- **Plugin Updates** - Keep plugins current; Nessus updates daily
- **Policy Tuning** - Customize scan policies based on asset type and risk tolerance
- **Bandwidth Management** - Limit concurrent hosts during business hours
- **Results Validation** - Manually verify critical findings before patching
- **Trend Analysis** - Track vulnerability metrics over time

<br>

## OpenVAS
OpenVAS (Open Vulnerability Assessment Scanner) is a fully-featured, open-source vulnerability scanner maintained by Greenbone Networks. It's part of the Greenbone Vulnerability Management (GVM) framework and provides enterprise-grade scanning capabilities at no cost.

### Key Features

- **90,000+ Network Vulnerability Tests (NVTs)** - Comprehensive vulnerability coverage
- **Daily Feed Updates** - Continuously updated vulnerability database
- **Compliance Checks** - Security policy auditing
- **Credentialed Scanning** - Authenticated local and remote checks
- **Performance-Based Scheduling** - Adaptive scanning based on network load
- **Flexible Architecture** - Modular design with separate scanner and manager
- **Web-Based Interface** - Greenbone Security Assistant (GSA)
- **API Access** - GMP (Greenbone Management Protocol) for automation

### Architecture Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Greenbone Security Assistant (GSA) â”‚  â† Web UI (Port 9392)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Greenbone Vulnerability Manager   â”‚  â† Management Layer
â”‚            (gvmd)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      OpenVAS Scanner (ospd)         â”‚  â† Scanning Engine
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<br>

### Installation & Setup

**Ubuntu/Debian (Recommended Method):**
```bash
# Add Greenbone PPA
sudo add-apt-repository ppa:mrazavi/gvm
sudo apt update

# Install GVM
sudo apt install gvm

# Setup and start services
sudo gvm-setup

# This will:
# 1. Create admin user
# 2. Download NVT feeds (takes 30-60 minutes)
# 3. Configure services
# 4. Display admin password

# Start GVM services
sudo gvm-start

# Access web interface
# URL: https://localhost:9392
# Username: admin
# Password: (displayed during setup)

# Check installation
sudo gvm-check-setup
```

<br>

**Docker Installation (Quick Start):**
```bash
# Use Greenbone Community Edition Docker container
docker run -d \
  --name openvas \
  -p 443:9392 \
  -e PASSWORD="SecureAdminPassword" \
  --volume openvas-data:/data \
  greenbone/openvas-scanner:latest

# Wait for initialization (check logs)
docker logs -f openvas

# Access at https://localhost
# Username: admin
# Password: SecureAdminPassword
```

**From Source (Advanced):**
```bash
# Install dependencies
sudo apt install -y \
  build-essential cmake pkg-config \
  libglib2.0-dev libgpgme-dev \
  libgnutls28-dev uuid-dev libssh-gcrypt-dev \
  libhiredis-dev libxml2-dev libpcap-dev \
  libnet1-dev libldap2-dev libradcli-dev

# Clone repositories
mkdir -p ~/gvm-source
cd ~/gvm-source

git clone https://github.com/greenbone/gvm-libs.git
git clone https://github.com/greenbone/gvmd.git
git clone https://github.com/greenbone/openvas-scanner.git
git clone https://github.com/greenbone/gsa.git

# Build and install each component
# (Detailed build instructions in official documentation)
```

**Initial Configuration:**
```bash
# Update NVT feed
sudo greenbone-nvt-sync

# Update SCAP data
sudo greenbone-scapdata-sync

# Update CERT data
sudo greenbone-certdata-sync

# Rebuild NVT cache
sudo openvas --update-vt-info

# Create a new user
sudo gvmd --create-user=scanner --password=ScannerPassword123

# Set user roles
sudo gvmd --modify-user=scanner --new-role=Admin
```

<br>

### Practical Examples

**Create and Launch Scan (Web UI):**
```
1. Login to GSA (https://localhost:9392)
2. Navigate to Scans > Tasks
3. Click the star icon (New Task)
4. Configure:
   - Name: "Weekly Infrastructure Scan"
   - Scan Targets: Create New Target
     - Name: "Production Servers"
     - Hosts: 192.168.1.0/24
     - Port List: All IANA assigned TCP and UDP
   - Scanner: OpenVAS Default
   - Scan Config: Full and fast
5. Save and Start
```

<br>

**Python API Automation with python-gvm:**

```
```python
from gvm.connections import TLSConnection
from gvm.protocols.gmp import Gmp
from gvm.transforms import EtreeTransform

# Configuration
hostname = 'localhost'
port = 9390
username = 'admin'
password = 'admin_password'

# Connect to GVM
connection = TLSConnection(hostname=hostname, port=port)
transform = EtreeTransform()

with Gmp(connection=connection, transform=transform) as gmp:
    # Authenticate
    gmp.authenticate(username, password)
    
    # Create target
    target_response = gmp.create_target(
        name='Web Servers',
        hosts=['192.168.1.10', '192.168.1.11', '192.168.1.12'],
        port_list_id='33d0cd82-57c6-11e1-8ed1-406186ea4fc5'  # All IANA TCP+UDP
    )
    target_id = target_response.get('id')
    print(f"Created target: {target_id}")
    
    # Create task
    config_id = 'daba56c8-73ec-11df-a475-002264764cea'  # Full and fast
    scanner_id = '08b69003-5fc2-4037-a479-93b440211c73'  # OpenVAS Default
    
    task_response = gmp.create_task(
        name='Automated Weekly Scan',
        config_id=config_id,
        target_id=target_id,
        scanner_id=scanner_id
    )
    task_id = task_response.get('id')
    print(f"Created task: {task_id}")
    
    # Start task
    gmp.start_task(task_id)
    print("Scan started")
    
    # Monitor progress
    import time
    while True:
        task_status = gmp.get_task(task_id)
        status = task_status.find('.//status').text
        progress = task_status.find('.//progress').text
        
        print(f"Status: {status}, Progress: {progress}%")
        
        if status == 'Done':
            break
        
        time.sleep(30)
    
    # Get results
    results = gmp.get_results(
        filter_string=f'task_id={task_id} and severity>0.0'
    )
    
    # Count vulnerabilities by severity
    severity_counts = {'High': 0, 'Medium': 0, 'Low': 0}
    for result in results.findall('.//result'):
        severity = float(result.find('.//severity').text)
        if severity >= 7.0:
            severity_counts['High'] += 1
        elif severity >= 4.0:
            severity_counts['Medium'] += 1
        else:
            severity_counts['Low'] += 1
    
    print(f"Vulnerabilities found: {severity_counts}")
```
```
**Export Scan Report:**
```python
from gvm.connections import TLSConnection
from gvm.protocols.gmp import Gmp
from gvm.transforms import EtreeTransform
import base64

connection = TLSConnection(hostname='localhost', port=9390)

with Gmp(connection=connection, transform=EtreeTransform()) as gmp:
    gmp.authenticate('admin', 'password')
    
    # Get report ID from completed task
    task_id = 'YOUR_TASK_ID'
    task = gmp.get_task(task_id)
    report_id = task.find('.//report').get('id')
    
    # Request report in PDF format
    report_format_id = 'c402cc3e-b531-11e1-9163-406186ea4fc5'  # PDF format
    
    response = gmp.get_report(
        report_id=report_id,
        report_format_id=report_format_id
    )
    
    # Extract and decode report
    report_element = response.find('.//report_format').getnext()
    content = report_element.text
    
    # Decode base64
    pdf_data = base64.b64decode(content)
    
    # Save to file
    with open('vulnerability_report.pdf', 'wb') as f:
        f.write(pdf_data)
    
    print("Report exported successfully")
```
```
**Scheduled Scanning with Cron:**
```bash
# Create scan script
cat > /usr/local/bin/openvas-weekly-scan.sh << 'EOF'
#!/bin/bash

# OpenVAS weekly scan automation
TASK_ID="your-task-id-here"
GVM_USER="admin"
GVM_PASS="admin_password"

# Start scan
gvm-cli --gmp-username=${GVM_USER} --gmp-password=${GVM_PASS} socket --xml "<start_task task_id='${TASK_ID}'/>"

# Log execution
echo "$(date): Weekly scan initiated (Task ID: ${TASK_ID})" >> /var/log/openvas-auto-scan.log
EOF

chmod +x /usr/local/bin/openvas-weekly-scan.sh

# Add to cron (Sundays at 2 AM)
(crontab -l 2>/dev/null; echo "0 2 * * 0 /usr/local/bin/openvas-weekly-scan.sh") | crontab -
```
```
**Integration with SIEM (Syslog Export):**
```python
from gvm.connections import TLSConnection
from gvm.protocols.gmp import Gmp
from gvm.transforms import EtreeTransform
import socket
import json

def send_to_siem(vulnerability_data, siem_host='siem.company.com', siem_port=514):
    """Send vulnerability data to SIEM via syslog"""
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    
    for vuln in vulnerability_data:
        message = f"<134>OpenVAS: {json.dumps(vuln)}"
        sock.sendto(message.encode(), (siem_host, siem_port))
    
    sock.close()

# Connect and fetch results
connection = TLSConnection(hostname='localhost', port=9390)

with Gmp(connection=connection, transform=EtreeTransform()) as gmp:
    gmp.authenticate('admin', 'password')
    
    # Get recent results (last 24 hours)
    results = gmp.get_results(
        filter_string='severity>7.0 and min_qod=70',
        details=True
    )
    
    vulnerability_data = []
    for result in results.findall('.//result'):
        vuln = {
            'host': result.find('.//host').text,
            'port': result.find('.//port').text,
            'severity': result.find('.//severity').text,
            'threat': result.find('.//threat').text,
            'name': result.find('.//name').text,
            'description': result.find('.//description').text,
            'cve': result.find('.//cve').text if result.find('.//cve') is not None else 'N/A'
        }
        vulnerability_data.append(vuln)
    
    # Send to SIEM
    send_to_siem(vulnerability_data)
    print(f"Sent {len(vulnerability_data)} high-severity vulnerabilities to SIEM")
```
```
### Best Practices

- **Regular Feed Updates** - Update NVT feeds daily for latest vulnerability checks
- **Scan Policies** - Use "Full and fast" for comprehensive scans, "Discovery" for asset inventory
- **Credentialed Scans** - Provide SSH/SMB credentials for accurate results
- **Network Optimization** - Use "Performance" scan configs for large networks
- **Report Management** - Archive old reports to maintain database performance
- **Resource Allocation** - Allocate sufficient CPU/RAM (minimum 4GB RAM)
- **Backup Strategy** - Regular backups of /var/lib/gvm database
- **User Management** - Create separate users for different teams/roles

---
```
```
## Qualys VMDR

### Overview

Qualys Vulnerability Management, Detection, and Response (VMDR) is a cloud-based platform that provides continuous visibility, assessment, and remediation of security vulnerabilities across on-premise, endpoint, cloud, and container environments.

### Key Features

- **Cloud-Native Architecture** - No infrastructure to maintain
- **Continuous Monitoring** - Real-time asset and vulnerability tracking
- **Global Sensor Network** - Distributed scanners for worldwide coverage
- **Asset Inventory** - Automatic discovery and classification
- **Patch Management Integration** - Automated remediation workflows
- **Compliance Reporting** - PCI-DSS, HIPAA, SOX, GDPR
- **TruRiskâ„¢** - Context-based risk scoring
- **API-First Design** - Extensive automation capabilities

### Deployment Models

**1. Cloud Agents:**
- Lightweight agents installed on endpoints
- Continuous assessment without network scanning
- Works for remote/mobile devices
- Real-time vulnerability detection

**2. Scanner Appliances:**
- Virtual or physical appliances
- Network-based vulnerability scanning
- Ideal for servers and network devices
- Scheduled or on-demand scans

**3. Cloud Connectors:**
- Agentless cloud asset discovery
- AWS, Azure, GCP, OCI support
- Container and Kubernetes scanning
- Serverless function assessment

### Installation & Setup

**Deploy Scanner Appliance:**
```bash
# Download OVA/VM image from Qualys portal
# Deploy to VMware, Hyper-V, or KVM

# Initial configuration via console
# 1. Set IP address, netmask, gateway
# 2. Set DNS servers
# 3. Configure Qualys platform URL
# 4. Enter personalization code (from Qualys portal)

# Verify connectivity
ping qualysapi.qualys.com

# Scanner will auto-register with Qualys cloud
```
```
**Deploy Cloud Agent (Linux):**
```bash
# Download agent installer from Qualys portal
wget https://YOUR_QUALYS_URL/CloudAgent/QualysCloudAgent.rpm

# Install agent
sudo rpm -ivh QualysCloudAgent.rpm

# Configure agent
sudo /usr/local/qualys/cloud-agent/bin/qualys-cloud-agent.sh \
  CustomerId=YOUR_CUSTOMER_ID \
  ActivationId=YOUR_ACTIVATION_ID

# Start agent
sudo systemctl start qualys-cloud-agent
sudo systemctl enable qualys-cloud-agent

# Verify agent status
sudo systemctl status qualys-cloud-agent
```
```
**Deploy Cloud Agent (Windows):**
```powershell
# Download installer
Invoke-WebRequest -Uri "https://YOUR_QUALYS_URL/CloudAgent/QualysCloudAgent.exe" -OutFile "C:\Temp\QualysCloudAgent.exe"

# Silent install
Start-Process -FilePath "C:\Temp\QualysCloudAgent.exe" -ArgumentList "CustomerId=YOUR_CUSTOMER_ID ActivationId=YOUR_ACTIVATION_ID /quiet" -Wait

# Verify installation
Get-Service -Name "QualysAgent"
```
```
**Configure AWS Connector:**
```bash
# In Qualys Portal:
# 1. Navigate to: CloudView > Connectors
# 2. Add Connector > Amazon Web Services
# 3. Select authentication method:
#    - IAM Role (recommended)
#    - Access Keys

# IAM Role method:
# Create IAM role in AWS with required permissions
# Attach Qualys policy:
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "QualysCloudViewPolicy",
      "Effect": "Allow",
      "Action": [
        "ec2:Describe*",
        "elasticloadbalancing:Describe*",
        "rds:Describe*",
        "s3:GetBucket*",
        "s3:ListAllMyBuckets",
        "lambda:List*",
        "iam:GetAccountSummary",
        "cloudtrail:LookupEvents"
      ],
      "Resource": "*"
    }
  ]
}
```
```
# Enter AWS Account ID and IAM Role ARN in Qualys
# Connector will begin discovering assets
```
```
### Practical Examples

**API Authentication:**
```python
import requests
from requests.auth import HTTPBasicAuth
import xml.etree.ElementTree as ET

class QualysAPI:
    def __init__(self, username, password, api_server='qualysapi.qualys.com'):
        self.username = username
        self.password = password
        self.base_url = f'https://{api_server}'
        self.session = requests.Session()
        self.session.auth = HTTPBasicAuth(username, password)
        self.session.headers.update({'X-Requested-With': 'Python'})
    
    def
```

<br>

---

<br>
<br>

<p align="center">

| â† Previous Page | Home | Next Phase â†’ |
|:-----------|:----:|-------:|
| [Preparation](./1.0-Preparation.md) | [Landing Page](../../README.md) | [Detection and Analysis](../2-Detection-and-Analysis/2.0-Detection-and-Analysis.md) |
